#https://docs.confluent.io/current/installation/docker/image-reference.html
---
version: '2.4'
services:

  #############################################################
  # Apache Zookeeper (CP)                                     #
  #############################################################
  zookeeper:
    image: confluentinc/cp-zookeeper:${VERSION_CONFLUENT}
    hostname: zookeeper
    container_name: zookeeper
    restart: always
    ports:
    - 12181:2181
    # https://docs.confluent.io/current/zookeeper/deployment.html
    # https://docs.confluent.io/current/installation/docker/config-reference.html#zk-configuration
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181 # (required) This is the port where ZooKeeper clients will listen on. This is where the Brokers will connect to ZooKeeper.
      ZOOKEEPER_TICK_TIME: 2000 # (default: 3000) The unit of time for ZooKeeper translated to milliseconds. This governs all ZooKeeper time dependent operations. It is used for heartbeats and timeouts especially.
    healthcheck:
      test: zookeeper-shell localhost:2181 ls / >/dev/null 2>&1 || exit 1
      start_period: 10s
      interval: 20s
      timeout: 18s
      retries: 10

  #############################################################
  # Confluent Server (CP)                                     #
  #############################################################
  kafka:
    image: confluentinc/cp-server:${VERSION_CONFLUENT}
    hostname: kafka
    container_name: kafka
    restart: always
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
    - 19092:19092
    # https://docs.confluent.io/current/installation/configuration/broker-configs.html
    # https://docs.confluent.io/current/installation/docker/config-reference.html#confluent-kafka-configuration
    environment:
      KAFKA_BROKER_ID: 1 # (default: -1) The broker id for this server. If unset, a unique broker id will be generated.
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 # (required) Instructs Kafka how to get in touch with ZooKeeper.
      KAFKA_CUB_ZK_TIMEOUT: 60 # (default: 40) Docker image setting, which specified the amount of time to wait for Zookeeper. Could be used, to get rid of Docker healthchecks. 
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: DOCKER:PLAINTEXT,HOST:PLAINTEXT # (default: PLAINTEXT:PLAINTEXT,...) Map between listener names and security protocols. In this scenario this setting is used to define listeners with names.
      KAFKA_LISTENERS: DOCKER://:9092,HOST://:19092 # (required) List of URIs we will listen on and the listener names. In this case, Kafka listens in both ports on all interfaces.
      KAFKA_ADVERTISED_LISTENERS: DOCKER://kafka.${DOMAIN_NAME}:9092,HOST://localhost:19092 # (required) Describes how the host name that is advertised and can be reached by clients. HOST://localhost:19092 enables access from Docker host.
      KAFKA_CONTROL_PLAIN_LISTENER_NAME: DOCKER # (default: PLAINTEXT) Name of listener used for communication between controller and brokers. By default, no dedicated listener is used.
      KAFKA_INTER_BROKER_LISTENER_NAME: DOCKER # (default: PLAINTEXT) Name of listener used for communication between brokers.  By default, no dedicated listener is used.
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0 # (default: 3000) The amount of time the group coordinator will wait for more consumers to join a new group before performing the first rebalance. Set to 0 to ensure, that consumers start faster in dev environments.
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false" # (default: true) We disabled auto creation of topics, to ensure that topics are created with the correct configuration. However, if defaults are fine, this could be enabled.
      KAFKA_NUM_PARTITIONS: 3 # (default: 1) The default number of partitions per topic.
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 # (default: 3) The replication factor for the offsets topic. Must be 1, because we only have a single broker.
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 # (default: 3) The replication factor for the transaction topic. Must be 1, because we only have a single broker.
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1 # (default: 2) Overridden min.insync.replicas config for the transaction topic. Must be 1, because we only have a single broker.
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1 # (default 3): The replication factor for the license topic. Must be 1, because we only have a single broker.
      KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081 # (default: unset) Allows activation of Schema Validation on the brokers for specific topics (see https://docs.confluent.io/current/schema-registry/schema-validation.html)
      # https://docs.confluent.io/current/kafka/metrics-reporter.html
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter # (default: "") A list of classes to use as metrics custom reporters. The definied reporter writes metrics to _confluent-metrics topic, which is required by Control Center.
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:9092 # (required) Point to Kafka brokers in the metrics cluster. In this scenario, metrics are not stored in a separate cluster.
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1 # (default: 3) Number of replicas in the metric topic. Must be 1, because we only have a single broker.
      CONFLUENT_METRICS_REPORTER_PUBLISHMS: 5000 # (default: 15000) The Confluent Metrics Reporter will publish new metrics to the metrics topic in intervals defined by this setting. Reduced for development, to reduce the health data lag.
      KAFKA_JMX_PORT: 9581
      KAFKA_JMX_HOSTNAME: kafka.showcase
    healthcheck:
      test: "(kafka-topics --bootstrap-server localhost:9092 --describe --topic _confluent-license | grep '_confluent-license.*Isr: [0-9]\\+' >/dev/null 2>&1) || exit 1"
      start_period: 20s
      interval: 30s
      timeout: 25s
      retries: 10

  #############################################################
  # Confluent Schema Registry (CP)                            #
  #############################################################
  schema-registry:
    image: confluentinc/cp-schema-registry:${VERSION_CONFLUENT}
    hostname: schema-registry
    container_name: schema-registry
    restart: always
    depends_on:
      kafka:
        condition: service_healthy
    ports:
    - 8081:8081  
    - 18081:8081
    # https://docs.confluent.io/current/schema-registry/installation/config.html
    # https://docs.confluent.io/current/installation/docker/config-reference.html#schema-registry-configuration
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry # (required) The advertised host name. Not reuqired in single host mode, but required by the image. 
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081 # (default: http://0.0.0.0:8081) Comma-separated list of listeners that listen for API requests
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092 # (required) A list of Kafka brokers to connect. If kafkastore.connection.url is not specified, the Kafka cluster containing these bootstrap servers is used both to coordinate Schema Registry instances (primary election) and to store schema data.
      SCHEMA_REGISTRY_CUB_KAFKA_TIMEOUT: 60 # (default: 40) Docker image setting, which specifies the amount of time to wait for Kafka. Could be used, to get rid of Docker healthchecks. 
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 1 # (default: 3) The desired replication factor of the schema topic. Must be 1, because we only have a single broker.
      SCHEMA_REGISTRY_AVRO_COMPATIBILITY_LEVEL: backward # (default: backward) The Avro compatibility type.
    healthcheck:
      test: test $$(curl -s -o /dev/null -w %{http_code}  http://localhost:8081/subjects) -eq 200 || exit 1
      start_period: 10s
      interval: 20s
      timeout: 18s
      retries: 10

  #############################################################
  # Kafka Connect (CP)                                        #
  #############################################################
  connect:
    image: novatec/cp-server-connect-showcase:${VERSION_CONFLUENT}
    build:
      context: kafkaconnect
      dockerfile: Dockerfile.connect
      args:
        VERSION_CONFLUENT: ${VERSION_CONFLUENT}
        CONNECTORS: |-
          confluentinc/kafka-connect-jdbc:10.0.0
    hostname: connect
    container_name: connect
    restart: always
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    ports:
    - 8083:8083
    - 18083:8083
    # https://docs.confluent.io/current/installation/configuration/connect/index.html
    # https://docs.confluent.io/current/installation/docker/config-reference.html#kafka-connect-configuration
    # https://docs.confluent.io/current/installation/configuration/connect/source-connect-configs.html
    # https://docs.confluent.io/current/installation/configuration/connect/sink-connect-configs.html
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092 # (required) A host:port pair for establishing the initial connection to the Kafka cluster.
      CONNECT_CUB_KAFKA_TIMEOUT: 60 # (default: 40) Docker image setting, which specifies the amount of time to wait for Kafka. Could be used, to get rid of Docker healthchecks. 
      CONNECT_REST_ADVERTISED_HOST_NAME: connect # (required) The hostname that is given out to other workers to connect to. Must be resolvable by all containers.
      CONNECT_REST_PORT: 8083 # (default: 8083) Port for the REST API to listen on.
      CONNECT_GROUP_ID: showcase_connect # (required) A unique string that identifies the Connect cluster group this worker belongs to.
      CONNECT_CONFIG_STORAGE_TOPIC: showcase_connect-configs # (required) The name of the topic in which to store connector and task configuration data. This must be the same for all workers with the same group.id
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1 # (default: 3) Replication factor used when creating the configuration storage topic. Must be 1, because we only have a single broker.
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000 # (default: 60000) Interval at which to try committing offsets for tasks. Explicitly reduced for dev environment.
      CONNECT_OFFSET_STORAGE_TOPIC: showcase_connect-offsets # (required) The name of the topic in which to store offset data for connectors. This must be the same for all workers with the same group.id
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1 # (default: 3) Replication factor used when creating the offset storage topic. Must be 1, because we only have a single broker.
      CONNECT_STATUS_STORAGE_TOPIC: showcase_connect-status # (required) The name of the topic in which to store state for connectors. This must be the same for all workers with the same group.id
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1 # (default: 3) Replication factor used when creating the status storage topic. Must be 1, because we only have a single broker.
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter # (required) Converter class for keys. This controls the format of the data that will be written to Kafka for source connectors or read from Kafka for sink connectors.
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter # (required) Converter class for values. This controls the format of the data that will be written to Kafka for source connectors or read from Kafka for sink connectors.
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081 # (required) Schema Registry Url which is used by AvroConvertor.
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter # (required) Converter class for internal keys.
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter # (required) Converter class for internal values.
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components,/etc/kafka-connect/jars # (default: /usr/share/java,/usr/share/confluent-hub-components) The plugin.path value that indicates the location from which to load Connect plugins in classloading isolation.
      CLASSPATH: /usr/share/java/kafka/*:/usr/share/java/monitoring-interceptors/* # CLASSPATH required due to CC-2422
      # https://docs.confluent.io/current/control-center/installation/clients.html#confluent-monitoring-interceptors
      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor
      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor
      # CONNECT_CONFLUENT_MONITORING_INTERCEPTOR_TOPIC: _confluent-monitoring # (default: _confluent-monitoring) Topic on which monitoring data will be written. The topic is created by Confluent Control Center.
      # CONNECT_CONFLUENT_MONITORING_INTERCEPTOR_PUBLISHMS: 15 # (default: 15) Period the interceptor should use to publish messages to.
      # https://docs.confluent.io/current/kafka/metrics-reporter.html
      CONNECT_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter # (default: "") A list of classes to use as metrics custom reporters. The definied reporter writes metrics to _confluent-metrics topic, which is required by Control Center.
      CONNECT_CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:9092 # (required) Point to Kafka brokers in the metrics cluster. In this scenario, metrics are not stored in a separate cluster.
      CONNECT_CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1 # (default: 3) Number of replicas in the metric topic. Must be 1, because we only have a single broker.
      CONNECT_CONFLUENT_METRICS_REPORTER_PUBLISH_MS: 5000 # (default: 15000) The Confluent Metrics Reporter will publish new metrics to the metrics topic in intervals defined by this setting. Reduced for development, to reduce the health data lag.
    healthcheck:
      test: test $$(curl -s -o /dev/null -w %{http_code} http://localhost:8083/connectors) -eq 200 || exit 1
      start_period: 60s
      interval: 20s
      timeout: 18s
      retries: 10

  #############################################################
  # Confluent ksqlDB Server                                   #
  #############################################################
  ksqldb-server:
    image: confluentinc/ksqldb-server:latest
    #build:
    #  context: ksqldb
    #  dockerfile: Dockerfile.ksqldb
    #  args:
    #   VERSION_CONFLUENT: ${VERSION_CONFLUENT}
    hostname: ksqldb-server
    container_name: ksqldb-server
    restart: always
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      connect:
        condition: service_healthy
    ports:
    - 8088:8088  
    - 18088:8088
    # https://docs.ksqldb.io/en/latest/operate-and-deploy/installation/server-config/config-reference/
    # https://docs.ksqldb.io/en/latest/operate-and-deploy/installation/install-ksqldb-with-docker/
    environment:
      ## ksqlDB Kafka Streams and Kafka Client Settings 
      ## https://docs.ksqldb.io/en/latest/operate-and-deploy/installation/server-config/config-reference/#kafka-streams-and-kafka-client-settings
      KSQL_BOOTSTRAP_SERVERS: kafka:9092 # (default: localhost:9092) list of host and port pairs that is used for establishing the initial connection to the Kafka cluster
      KSQL_KSQL_STREAMS_AUTO_OFFSET_RESET: "earliest" # (default: latest) determines what to do when there is no initial offset in Apache Kafka
      KSQL_KSQL_STREAMS_NUM_STREAM_THREADS: 1 # (default: 1) number of stream threads in an instance of the Kafka Streams application
      KSQL_KSQL_STREAMS_PRODUCER_DELIVERY_TIMEOUT_MS: 2147483647 # (default: 120000) Set the batch expiry to Integer.MAX_VALUE to ensure that queries will not terminate if the underlying Kafka cluster is unavailable
      KSQL_KSQL_STREAMS_PRODUCER_MAX_BLOCK_MS: 9223372036854775807 # (default: 60000) Set the maximum allowable time for the producer to block to Long.MAX_VALUE. This allows ksqlDB to pause processing if the underlying Kafka cluster is unavailable. 
      # Data is flushed to the state store and forwarded to the next downstream processor node whenever the earliest of commit.interval.ms or cache.max.bytes.buffering (cache pressure) hits.
      # Therfore, we set cache.max.bytes.buffering to 0, to ensure that every single record is set downstream, which can be helpful for debuging. Don't do this in production environments with high throughput.
      KSQL_KSQL_STREAMS_COMMIT_INTERVAL_MS: 2000 # (default: 2000) frequency to save the position of the processor
      KSQL_KSQL_STREAMS_CACHE_MAX_BYTES_BUFFERING: 0 # (default: 10000000) maximum number of memory bytes to be used for buffering across all threads   
      ## ksqlDB Query Settings
      ## https://docs.ksqldb.io/en/latest/operate-and-deploy/installation/server-config/config-reference/#ksqldb-query-settings
      KSQL_KSQL_SERVICE_ID: showcase_ksql_ # (default: default_) used to define the ksqlDB cluster membership of a ksqlDB Server instance
      KSQL_KSQL_OUTPUT_TOPIC_NAME_PREFIX: "" # (default: "") default prefix for automatically created topic names
      KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry:8081 # (required for Avro) Schema Registry URL path to connect ksqlDB to
      ## ksqlDB Server Settings
      ## https://docs.ksqldb.io/en/latest/operate-and-deploy/installation/server-config/config-reference/#ksqldb-server-settings
      KSQL_LISTENERS: http://0.0.0.0:8088 # (default: http://0.0.0.0:8088) controls the REST API endpoint for the ksqlDB Server
      KSQL_KSQL_STREAMS_STATE_DIR: /tmp/kafka-streams # (default: /tmp/kafka-streams) the storage directory for stateful operations, like aggregations and joins
      KSQL_KSQL_EXTENSION_DIR: /etc/ksql/ext # extension directory for UDFs, see https://docs.ksqldb.io/en/latest/developer-guide/implement-a-udf/
      ## ksqlDB Connect Settings
      KSQL_KSQL_CONNECT_URL: http://connect:8083 # The Connect cluster URL to integrate with.
      KSQL_KSQL_CONNECT_POLLING_ENABLE: "true" # Toggles whether or not to poll connect for new connectors and automatically register them in ksqlDB.
      # ksqlDB Monitoring Settings (https://docs.confluent.io/current/control-center/installation/clients.html#confluent-monitoring-interceptors)
      KSQL_CONSUMER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor,de.novatec.kafka.streams.interceptors.TracingConsumerInterceptor
      KSQL_PRODUCER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor,de.novatec.kafka.streams.interceptors.TracingProducerInterceptor
      # KSQL_CONFLUENT_MONITORING_INTERCEPTOR_TOPIC: _confluent-monitoring # (default: _confluent-monitoring) Topic on which monitoring data will be written. The topic is created by Confluent Control Center.
      # KSQL_CONFLUENT_MONITORING_INTERCEPTOR_PUBLISHMS: 15 # (default: 15) Period the interceptor should use to publish messages to.
    healthcheck:
      test: test $$(curl -s -o /dev/null -w %{http_code} http://localhost:8088/healthcheck) -eq 200 || exit 1
      start_period: 45s
      interval: 20s
      timeout: 18s
      retries: 10

  #############################################################
  # Confluent Control Center (CP)                             #
  #############################################################
  control-center:
    image: confluentinc/cp-enterprise-control-center:${VERSION_CONFLUENT}
    hostname: control-center
    container_name: control-center
    restart: always
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      connect:
        condition: service_healthy
      ksqldb-server:
        condition: service_healthy
    ports:
    - 9021:9021
    - 19021:9021
    # https://docs.confluent.io/current/control-center/installation/configuration.html
    # https://docs.confluent.io/current/installation/docker/config-reference.html#c3-configuration
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: kafka:9092 # (required) A host:port pair for establishing the initial connection to the Kafka cluster. Multiple bootstrap servers can be used in the form host1:port1,host2:port2,host3:port3....
      CONTROL_CENTER_REPLICATION_FACTOR: 1 # (required) Replication factor for Control Center topics.
      CONTROL_CENTER_CUB_KAFKA_TIMEOUT: 300 # (default: 300) # Docker images waits for this amount of time until at least CONTROL_CENTER_REPLICATION_FACTOR brokers are alive.
      CONTROL_CENTER_REST_LISTENERS: http://0.0.0.0:9021 # Set this to the HTTP or HTTPS of Control Center UI.
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081 # Schema Registry URL
      CONTROL_CENTER_CONNECT_SHOWCASE_CLUSTER: http://connect:8083 # Comma-separated list of Kafka Connect worker URLs for the Connect cluster
      CONTROL_CENTER_KSQL_SHOWCASE_URL: http://ksqldb-server:8088  # Comma-separated list of the ksqlDB server hostnames and listener ports for the ksqlDB cluster
      CONTROL_CENTER_KSQL_SHOWCASE_ADVERTISED_URL: http://localhost:8088 # Comma-separated list of advertised URLs to access the ksqlDB cluster on Control Center. These hostnames must be reachable from any browser that will use the ksqlDB web interface in Control Center.
      CONTROL_CENTER_SERVICE_HEALTHCHECK_INTERVAL_SEC: 15 # The interval (in seconds) used for checking the health of Confluent Platform nodes. This includes ksqlDB, Connect, Schema Registry, REST Proxy, and Metadata Service (MDS).
    healthcheck:
      test: test $$(curl -s -o /dev/null -w %{http_code} http://localhost:9021/2.0/alerts/history/) -eq 200 || exit 1
      start_period: 45s
      interval: 20s
      timeout: 18s
      retries: 10


  #############################################################
  # PostgreSQL                                                #
  #############################################################
  postgres_db:
    image: postgres:latest
    hostname: postgres_db
    container_name: postgres_db
    ports:
      - 5432:5432
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: test
      POSTGRES_DB: tc_showcase


networks:
  default:
    name: ${DOMAIN_NAME}
